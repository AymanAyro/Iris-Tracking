{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 05:47:39.426541: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-26 05:47:39.714988: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-26 05:47:39.793958: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-26 05:47:40.388444: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-26 05:47:43.275756: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "from comet_ml import Experiment\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input as efficientnet_preprocess_input\n",
    "from tensorflow.keras.applications.resnet_v2 import preprocess_input as resnet_preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = r\"/home/ayman/AI Projects/Iris Detection/iris_data/aug_data/train\"\n",
    "test_dir = r\"/home/ayman/AI Projects/Iris Detection/iris_data/aug_data/test\"\n",
    "val_dir = r\"/home/ayman/AI Projects/Iris Detection/iris_data/aug_data/val\"\n",
    "train_images_dir = os.path.join(train_dir, \"images\")\n",
    "train_labels_dir = os.path.join(train_dir, \"labels\")\n",
    "test_images_dir = os.path.join(test_dir, \"images\")\n",
    "test_labels_dir = os.path.join(test_dir, \"labels\")\n",
    "val_images_dir = os.path.join(val_dir, \"images\")\n",
    "val_labels_dir = os.path.join(val_dir, \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path, resolution, model_type='efficientnet'):\n",
    "    # Load and preprocess image\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(image, [resolution, resolution])\n",
    "    \n",
    "    if model_type == 'efficientnet':\n",
    "        image = efficientnet_preprocess_input(image)\n",
    "    elif model_type == 'resnet':\n",
    "        image = resnet_preprocess_input(image)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported model type: {model_type}\")\n",
    "    \n",
    "    return image\n",
    "\n",
    "\n",
    "def load_label(label_path):\n",
    "    def parse_json(file_path):\n",
    "        with open(file_path.decode('utf-8'), 'r') as f:\n",
    "            data = json.load(f)\n",
    "            return np.array(data['keypoints'], dtype=np.float16)\n",
    "\n",
    "    # Use tf.numpy_function to wrap the custom function\n",
    "    label = tf.numpy_function(parse_json, [label_path], tf.float16)\n",
    "    # Specify the output shape\n",
    "    label.set_shape([4])  # Adjust based on your actual label size\n",
    "\n",
    "    return label\n",
    "\n",
    "def data_generator(image_paths, label_paths, batch_size=32, augment=False, shuffle=True):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((image_paths, label_paths))\n",
    "    \n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=len(image_paths))\n",
    "    \n",
    "    dataset = dataset.map(lambda x, y: (load_image(x, resolution=resolution), load_label(y)), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    if augment:\n",
    "        def augment(image, label):\n",
    "            image = tf.image.random_flip_left_right(image)\n",
    "            image = tf.image.random_contrast(image, 0.8, 1.2)\n",
    "            image = tf.image.random_brightness(image, 0.2)\n",
    "            return image, label\n",
    "        dataset = dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    \n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def get_sorted_file_paths(image_dir, label_dir):\n",
    "    image_paths = sorted([os.path.join(image_dir, file) for file in os.listdir(image_dir) if file.endswith('.png') or file.endswith('.jpg')])\n",
    "    label_paths = sorted([os.path.join(label_dir, file) for file in os.listdir(label_dir) if file.endswith('.json')])\n",
    "    \n",
    "    if len(image_paths) != len(label_paths):\n",
    "        raise ValueError(\"Number of images and labels do not match\")\n",
    "    \n",
    "    return image_paths, label_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7250, 7250)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir(train_images_dir)), len(os.listdir(train_labels_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1724640471.351758  204523 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1724640472.083324  204523 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1724640472.083533  204523 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1724640472.089013  204523 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1724640472.089167  204523 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1724640472.089201  204523 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1724640472.557097  204523 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "I0000 00:00:1724640472.557361  204523 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-26 05:47:52.557376: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2112] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "I0000 00:00:1724640472.557467  204523 cuda_executor.cc:1001] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2024-08-26 05:47:52.559383: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2021] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 9711 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3060, pci bus id: 0000:01:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "train_image_paths, train_label_paths = get_sorted_file_paths(train_images_dir, train_labels_dir)\n",
    "test_image_paths, test_label_paths = get_sorted_file_paths(test_images_dir, test_labels_dir)\n",
    "val_image_paths, val_label_paths = get_sorted_file_paths(val_images_dir, val_labels_dir)\n",
    "\n",
    "train_gen = data_generator(train_image_paths, train_label_paths, shuffle=True, batch_size=batch_size)\n",
    "test_gen = data_generator(test_image_paths, test_label_paths, shuffle=False, batch_size=batch_size)\n",
    "val_gen = data_generator(val_image_paths, val_label_paths, shuffle=False, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_iter = iter(train_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = next(data_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorShape([100, 224, 224, 3]), TensorShape([100, 4]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4,), dtype=float16, numpy=array([0.361 , 0.3328, 0.4385, 0.3574], dtype=float16)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image path: /home/ayman/AI Projects/Iris Detection/iris_data/aug_data/train/images/26f54047-5bda-11ef-a436-c18859564890.0.png\n",
      "Label: tf.Tensor([0.361  0.3328 0.4385 0.3574], shape=(4,), dtype=float16)\n",
      "Image path: /home/ayman/AI Projects/Iris Detection/iris_data/aug_data/train/images/26f54047-5bda-11ef-a436-c18859564890.1.png\n",
      "Label: tf.Tensor([0.2267 0.292  0.3416 0.266 ], shape=(4,), dtype=float16)\n",
      "Image path: /home/ayman/AI Projects/Iris Detection/iris_data/aug_data/train/images/26f54047-5bda-11ef-a436-c18859564890.10.png\n",
      "Label: tf.Tensor([0.3198 0.3606 0.2423 0.4114], shape=(4,), dtype=float16)\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(\"Image path:\", train_image_paths[i])\n",
    "    print(\"Label:\", y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(224, 224, 3), dtype=float32, numpy=\n",
       "array([[[102.53571 , 103.67729 , 107.761475],\n",
       "        [117.70153 , 124.386475, 128.13521 ],\n",
       "        [137.54208 , 142.67091 , 148.84311 ],\n",
       "        ...,\n",
       "        [187.46184 , 193.60704 , 183.93727 ],\n",
       "        [189.92616 , 191.75122 , 195.14044 ],\n",
       "        [192.20003 , 194.94533 , 198.40291 ]],\n",
       "\n",
       "       [[103.60332 ,  99.41581 , 107.688774],\n",
       "        [115.78189 , 123.19388 , 124.      ],\n",
       "        [136.99362 , 143.3699  , 145.02296 ],\n",
       "        ...,\n",
       "        [180.65329 , 192.50636 , 190.89958 ],\n",
       "        [189.50758 , 195.33423 , 198.4235  ],\n",
       "        [190.92728 , 195.25137 , 196.06381 ]],\n",
       "\n",
       "       [[ 94.46938 ,  99.97832 , 102.34311 ],\n",
       "        [111.13265 , 117.11224 , 118.55485 ],\n",
       "        [133.11862 , 139.14668 , 144.78444 ],\n",
       "        ...,\n",
       "        [185.28702 , 193.28812 , 192.60303 ],\n",
       "        [190.00876 , 193.77704 , 197.52931 ],\n",
       "        [192.88875 , 193.45923 , 200.75119 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 54.226902,  54.156986,  55.205738],\n",
       "        [ 53.916004,  54.24734 ,  56.544735],\n",
       "        [ 53.7232  ,  51.255005,  58.502502],\n",
       "        ...,\n",
       "        [132.52644 , 165.89435 , 202.72227 ],\n",
       "        [132.14021 , 163.54478 , 204.52026 ],\n",
       "        [133.62349 , 163.69148 , 196.45706 ]],\n",
       "\n",
       "       [[ 57.381245,  52.83151 ,  57.60846 ],\n",
       "        [ 55.352192,  50.686325,  56.618668],\n",
       "        [ 54.02296 ,  50.135014,  55.67721 ],\n",
       "        ...,\n",
       "        [132.30309 , 167.16321 , 202.84822 ],\n",
       "        [132.41805 , 162.7933  , 198.04361 ],\n",
       "        [129.10718 , 159.88132 , 202.74207 ]],\n",
       "\n",
       "       [[ 52.417126,  49.99108 ,  55.287014],\n",
       "        [ 52.650513,  54.830315,  56.009056],\n",
       "        [ 53.55868 ,  50.60096 ,  58.212883],\n",
       "        ...,\n",
       "        [132.5062  , 169.62227 , 203.42374 ],\n",
       "        [129.67627 , 164.1836  , 199.93759 ],\n",
       "        [131.48709 , 160.7359  , 193.83444 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n",
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image 0: Coord1 (80, 74), Coord2 (98, 80)\n",
      "Image 1: Coord1 (50, 65), Coord2 (76, 59)\n",
      "Image 2: Coord1 (71, 80), Coord2 (54, 92)\n",
      "Image 3: Coord1 (35, 87), Coord2 (53, 79)\n",
      "Image 4: Coord1 (126, 87), Coord2 (107, 78)\n",
      "Image 5: Coord1 (105, 57), Coord2 (83, 43)\n",
      "Image 6: Coord1 (31, 81), Coord2 (45, 74)\n",
      "Image 7: Coord1 (69, 76), Coord2 (87, 68)\n",
      "Image 8: Coord1 (79, 60), Coord2 (94, 65)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x800 with 9 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Reverse the EfficientNet preprocessing\n",
    "def reverse_efficientnet_preprocess(image):\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    image = (image * std) + mean  # Reverse the normalization\n",
    "    image = np.clip(image * 255, 0, 255).astype(np.uint8)\n",
    "    return image # Convert back to [0, 255]\n",
    "\n",
    "# Adjust the coordinates to match the original image resolution\n",
    "def correct_coordinates(label, image_size=(resolution, resolution)):\n",
    "    return tuple(np.multiply(label, image_size).astype(int))\n",
    "\n",
    "# Plotting the images and the keypoints\n",
    "fig, ax = plt.subplots(3, 3, figsize=(8, 8))\n",
    "\n",
    "# Choose the preprocessing method\n",
    "model_type = 'efficientnet'  # or 'resnet' depending on your original model\n",
    "\n",
    "for i in range(9):\n",
    "    sample_image = X_train[i].numpy()\n",
    "    # Reverse the preprocessing\n",
    "    # sample_image = reverse_efficientnet_preprocess(sample_image)\n",
    "    \n",
    "    sample_image = np.array(sample_image, dtype=np.float32)\n",
    "    \n",
    "    sample_label = y_train[i].numpy()\n",
    "    \n",
    "    # Correct the coordinates based on original resolution\n",
    "    coord1 = correct_coordinates(sample_label[0:2])\n",
    "    coord2 = correct_coordinates(sample_label[2:4])\n",
    "    \n",
    "    print(f\"Image {i}: Coord1 {coord1}, Coord2 {coord2}\")\n",
    "    \n",
    "    # Draw circles for the keypoints\n",
    "    cv2.circle(sample_image, coord1, 3, (255, 0, 0), -1)\n",
    "    cv2.circle(sample_image, coord2, 3, (0, 0, 255), -1)\n",
    "    \n",
    "    ax[i//3, i%3].imshow(sample_image)\n",
    "    ax[i//3, i%3].axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.applications import EfficientNetB3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Backbone Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "backbone = EfficientNetB3(weights='imagenet', include_top=False, input_tensor=layers.Input(batch_shape=(None, resolution, resolution, 3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Full Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ayman/.local/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "# freeze layers\n",
    "# for layer in backbone.layers[:-31]:\n",
    "#     layer.trainable = False\n",
    "backbone.layers[0] = layers.Conv2D(40, 3,  padding='same', activation='relu', input_shape=(resolution, resolution, 3), use_bias=False)\n",
    "\n",
    "model = models.Sequential()\n",
    "x = backbone.output\n",
    "x = layers.Conv2D(256, 3, padding='same' ,activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "# x = layers.Conv2D(512, 3, padding='same' ,activation='relu')(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2D(128, 3, padding='same' ,activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "# x = layers.Conv2D(256, 3, padding='same' ,activation='relu')(x)\n",
    "# x = layers.BatchNormalization()(x)\n",
    "x = layers.Conv2D(64, 3,  padding='same' ,activation='relu')(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "# x = layers.Conv2D(128, 3,2, padding='same' ,activation='relu')(x)\n",
    "\n",
    "x = layers.Dropout(0.5)(x)\n",
    "x = layers.Conv2D(4, (2, 2), activation='linear')(x)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "\n",
    "\n",
    "model = models.Model(backbone.input, x, name='Iris_Detection_ModelV4')\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Compiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set policy\n",
    "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.AdamW(learning_rate=1e-3, beta_1=0.9, beta_2=0.937, weight_decay=0.007)\n",
    "optimizer = tf.keras.mixed_precision.LossScaleOptimizer(optimizer)\n",
    "loss = tf.keras.losses.Huber(delta=0.2)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['mse', 'mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_schedule_with_warmup(epoch):\n",
    "    min_lr = 1e-5\n",
    "    max_lr = 1e-3\n",
    "    warmup_epochs = 10\n",
    "    total_epochs = 80  # Set this to the total number of epochs for your training\n",
    "\n",
    "    if epoch < warmup_epochs:\n",
    "        # Linear warmup\n",
    "        lr = (max_lr - min_lr) / warmup_epochs * epoch + min_lr\n",
    "    else:\n",
    "        # Cosine annealing after warmup\n",
    "        progress = (epoch - warmup_epochs) / (total_epochs - warmup_epochs)\n",
    "        lr = min_lr + 0.5 * (max_lr - min_lr) * (1 + np.cos(progress * np.pi))\n",
    "    \n",
    "    return lr\n",
    "\n",
    "# Create a callback for the learning rate schedule\n",
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(cosine_schedule_with_warmup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# steps_per_epoch = len(train_image_paths) // batch_size\n",
    "# validation_steps = len(val_image_paths) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: keras, tensorflow.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/aymansab0/iris-detection/594ee119b93a42e4b6d761091982cdf3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = Experiment(\n",
    "    api_key=\"yIHOp6Gk4OJB5NKzKJ2S1v1nu\",\n",
    "    project_name=\"iris-detection\",\n",
    ")\n",
    "writer = tf.summary.create_file_writer(\"./logs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback\n",
    "early_stopping_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, restore_best_weights=True)\n",
    "# Model checkpoint callback\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath='best_model_2.keras',\n",
    "    save_best_only=True,\n",
    "    monitor='val_loss',\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/home/ayman/AI Projects/Iris Detection/iris_data' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1724640547.128364  204610 service.cc:146] XLA service 0x7f79dc003460 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "I0000 00:00:1724640547.128559  204610 service.cc:154]   StreamExecutor device (0): NVIDIA GeForce RTX 3060, Compute Capability 8.6\n",
      "2024-08-26 05:49:08.060312: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-08-26 05:49:16.205839: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:531] Loaded cuDNN version 8907\n",
      "2024-08-26 05:49:56.494734: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng46{k2=1,k5=2,k14=5} for conv (f32[100,144,113,113]{3,2,1,0}, u8[0]{0}) custom-call(f32[100,144,115,115]{3,2,1,0}, f32[144,1,3,3]{3,2,1,0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, feature_group_count=144, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2024-08-26 05:49:57.001976: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 1.50914674s\n",
      "Trying algorithm eng46{k2=1,k5=2,k14=5} for conv (f32[100,144,113,113]{3,2,1,0}, u8[0]{0}) custom-call(f32[100,144,115,115]{3,2,1,0}, f32[144,1,3,3]{3,2,1,0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, feature_group_count=144, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2024-08-26 05:49:58.003242: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng46{k2=0,k5=2,k14=4} for conv (f32[100,144,113,113]{3,2,1,0}, u8[0]{0}) custom-call(f32[100,144,115,115]{3,2,1,0}, f32[144,1,3,3]{3,2,1,0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, feature_group_count=144, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2024-08-26 05:49:59.365151: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 2.363064384s\n",
      "Trying algorithm eng46{k2=0,k5=2,k14=4} for conv (f32[100,144,113,113]{3,2,1,0}, u8[0]{0}) custom-call(f32[100,144,115,115]{3,2,1,0}, f32[144,1,3,3]{3,2,1,0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, feature_group_count=144, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2024-08-26 05:50:00.366134: E external/local_xla/xla/service/slow_operation_alarm.cc:65] Trying algorithm eng46{k2=1,k5=3,k14=2} for conv (f32[100,144,113,113]{3,2,1,0}, u8[0]{0}) custom-call(f32[100,144,115,115]{3,2,1,0}, f32[144,1,3,3]{3,2,1,0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, feature_group_count=144, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "2024-08-26 05:50:01.721189: E external/local_xla/xla/service/slow_operation_alarm.cc:133] The operation took 2.355871434s\n",
      "Trying algorithm eng46{k2=1,k5=3,k14=2} for conv (f32[100,144,113,113]{3,2,1,0}, u8[0]{0}) custom-call(f32[100,144,115,115]{3,2,1,0}, f32[144,1,3,3]{3,2,1,0}), window={size=3x3}, dim_labels=bf01_oi01->bf01, feature_group_count=144, custom_call_target=\"__cudnn$convForward\", backend_config={\"operation_queue_id\":\"0\",\"wait_on_operation_queues\":[],\"cudnn_conv_backend_config\":{\"conv_result_scale\":1,\"activation_mode\":\"kNone\",\"side_input_scale\":0,\"leakyrelu_alpha\":0},\"force_earliest_schedule\":false} is taking a while...\n",
      "E0000 00:00:1724640626.301123  204610 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1724640626.563344  204610 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1724640626.825604  204610 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "I0000 00:00:1724640689.756239  204610 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m72/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 485ms/step - loss: 0.0705 - mae: 0.4430 - mse: 0.3141"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1724640777.076903  204610 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n",
      "E0000 00:00:1724640777.344505  204610 gpu_timer.cc:183] Delay kernel timed out: measured time has sub-optimal accuracy. There may be a missing warmup execution, please investigate in Nsight Systems.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 0.0704 - mae: 0.4422 - mse: 0.3131   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 05:54:29.103419: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 849346560 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1: val_loss improved from inf to 0.03683, saving model to best_model_2.keras\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m397s\u001b[0m 3s/step - loss: 0.0702 - mae: 0.4415 - mse: 0.3123 - val_loss: 0.0368 - val_mae: 0.2741 - val_mse: 0.1005 - learning_rate: 1.0000e-05\n",
      "Epoch 2/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 519ms/step - loss: 0.0370 - mae: 0.2687 - mse: 0.1205"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 05:55:31.655715: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 849346560 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2: val_loss did not improve from 0.03683\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 661ms/step - loss: 0.0369 - mae: 0.2682 - mse: 0.1201 - val_loss: 0.0503 - val_mae: 0.3344 - val_mse: 0.2238 - learning_rate: 1.0900e-04\n",
      "Epoch 3/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 476ms/step - loss: 0.0162 - mae: 0.1454 - mse: 0.0678"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 05:56:17.684717: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 849346560 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3: val_loss improved from 0.03683 to 0.00709, saving model to best_model_2.keras\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 805ms/step - loss: 0.0162 - mae: 0.1451 - mse: 0.0678 - val_loss: 0.0071 - val_mae: 0.0963 - val_mse: 0.0279 - learning_rate: 2.0800e-04\n",
      "Epoch 4/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 475ms/step - loss: 0.0087 - mae: 0.0910 - mse: 0.0616"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 05:57:18.280108: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 849346560 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4: val_loss improved from 0.00709 to 0.00345, saving model to best_model_2.keras\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 793ms/step - loss: 0.0087 - mae: 0.0909 - mse: 0.0615 - val_loss: 0.0035 - val_mae: 0.0624 - val_mse: 0.0084 - learning_rate: 3.0700e-04\n",
      "Epoch 5/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 478ms/step - loss: 0.0076 - mae: 0.0796 - mse: 0.0635"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 05:58:21.548328: W external/local_tsl/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 849346560 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5: val_loss improved from 0.00345 to 0.00156, saving model to best_model_2.keras\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m61s\u001b[0m 826ms/step - loss: 0.0076 - mae: 0.0796 - mse: 0.0635 - val_loss: 0.0016 - val_mae: 0.0356 - val_mse: 0.0035 - learning_rate: 4.0600e-04\n",
      "Epoch 6/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482ms/step - loss: 0.0060 - mae: 0.0664 - mse: 0.0439\n",
      "Epoch 6: val_loss did not improve from 0.00156\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 619ms/step - loss: 0.0060 - mae: 0.0664 - mse: 0.0439 - val_loss: 0.0030 - val_mae: 0.0558 - val_mse: 0.0072 - learning_rate: 5.0500e-04\n",
      "Epoch 7/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486ms/step - loss: 0.0065 - mae: 0.0692 - mse: 0.0536\n",
      "Epoch 7: val_loss did not improve from 0.00156\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 622ms/step - loss: 0.0065 - mae: 0.0692 - mse: 0.0535 - val_loss: 0.0019 - val_mae: 0.0391 - val_mse: 0.0044 - learning_rate: 6.0400e-04\n",
      "Epoch 8/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 474ms/step - loss: 0.0054 - mae: 0.0601 - mse: 0.0409\n",
      "Epoch 8: val_loss did not improve from 0.00156\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 607ms/step - loss: 0.0054 - mae: 0.0601 - mse: 0.0408 - val_loss: 0.0021 - val_mae: 0.0397 - val_mse: 0.0048 - learning_rate: 7.0300e-04\n",
      "Epoch 9/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 487ms/step - loss: 0.0039 - mae: 0.0505 - mse: 0.0248\n",
      "Epoch 9: val_loss did not improve from 0.00156\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 625ms/step - loss: 0.0039 - mae: 0.0505 - mse: 0.0249 - val_loss: 0.0020 - val_mae: 0.0454 - val_mse: 0.0048 - learning_rate: 8.0200e-04\n",
      "Epoch 10/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484ms/step - loss: 0.0039 - mae: 0.0487 - mse: 0.0260\n",
      "Epoch 10: val_loss did not improve from 0.00156\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 606ms/step - loss: 0.0039 - mae: 0.0487 - mse: 0.0260 - val_loss: 0.0023 - val_mae: 0.0496 - val_mse: 0.0054 - learning_rate: 9.0100e-04\n",
      "Epoch 11/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485ms/step - loss: 0.0034 - mae: 0.0432 - mse: 0.0205\n",
      "Epoch 11: val_loss did not improve from 0.00156\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 619ms/step - loss: 0.0034 - mae: 0.0432 - mse: 0.0205 - val_loss: 0.3053 - val_mae: 1.5539 - val_mse: 178.2845 - learning_rate: 0.0010\n",
      "Epoch 12/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484ms/step - loss: 0.0030 - mae: 0.0392 - mse: 0.0171"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:03:45.097876: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 12: val_loss did not improve from 0.00156\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 607ms/step - loss: 0.0030 - mae: 0.0392 - mse: 0.0171 - val_loss: 0.0022 - val_mae: 0.0403 - val_mse: 0.0059 - learning_rate: 9.9950e-04\n",
      "Epoch 13/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483ms/step - loss: 0.0028 - mae: 0.0378 - mse: 0.0159\n",
      "Epoch 13: val_loss did not improve from 0.00156\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 678ms/step - loss: 0.0028 - mae: 0.0377 - mse: 0.0159 - val_loss: 0.0016 - val_mae: 0.0286 - val_mse: 0.0039 - learning_rate: 9.9801e-04\n",
      "Epoch 14/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483ms/step - loss: 0.0025 - mae: 0.0331 - mse: 0.0133"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:05:19.852136: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 14: val_loss improved from 0.00156 to 0.00148, saving model to best_model_2.keras\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 791ms/step - loss: 0.0025 - mae: 0.0331 - mse: 0.0132 - val_loss: 0.0015 - val_mae: 0.0277 - val_mse: 0.0035 - learning_rate: 9.9552e-04\n",
      "Epoch 15/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489ms/step - loss: 0.0019 - mae: 0.0295 - mse: 0.0083"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:06:21.305906: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 15: val_loss improved from 0.00148 to 0.00107, saving model to best_model_2.keras\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 776ms/step - loss: 0.0019 - mae: 0.0295 - mse: 0.0083 - val_loss: 0.0011 - val_mae: 0.0252 - val_mse: 0.0027 - learning_rate: 9.9205e-04\n",
      "Epoch 16/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482ms/step - loss: 0.0018 - mae: 0.0289 - mse: 0.0074\n",
      "Epoch 16: val_loss did not improve from 0.00107\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 620ms/step - loss: 0.0018 - mae: 0.0289 - mse: 0.0074 - val_loss: 0.0149 - val_mae: 0.0967 - val_mse: 0.6264 - learning_rate: 9.8759e-04\n",
      "Epoch 17/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483ms/step - loss: 0.0018 - mae: 0.0282 - mse: 0.0075\n",
      "Epoch 17: val_loss did not improve from 0.00107\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 674ms/step - loss: 0.0018 - mae: 0.0283 - mse: 0.0075 - val_loss: 0.0013 - val_mae: 0.0288 - val_mse: 0.0032 - learning_rate: 9.8216e-04\n",
      "Epoch 18/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467ms/step - loss: 0.0015 - mae: 0.0268 - mse: 0.0047\n",
      "Epoch 18: val_loss did not improve from 0.00107\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 586ms/step - loss: 0.0015 - mae: 0.0268 - mse: 0.0047 - val_loss: 0.0012 - val_mae: 0.0248 - val_mse: 0.0029 - learning_rate: 9.7577e-04\n",
      "Epoch 19/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464ms/step - loss: 0.0012 - mae: 0.0241 - mse: 0.0037\n",
      "Epoch 19: val_loss did not improve from 0.00107\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 750ms/step - loss: 0.0012 - mae: 0.0241 - mse: 0.0037 - val_loss: 0.0099 - val_mae: 0.0724 - val_mse: 0.3371 - learning_rate: 9.6844e-04\n",
      "Epoch 20/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467ms/step - loss: 0.0011 - mae: 0.0228 - mse: 0.0030"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:10:32.776310: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 20: val_loss did not improve from 0.00107\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 586ms/step - loss: 0.0011 - mae: 0.0228 - mse: 0.0030 - val_loss: 0.0011 - val_mae: 0.0224 - val_mse: 0.0028 - learning_rate: 9.6017e-04\n",
      "Epoch 21/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 463ms/step - loss: 9.4911e-04 - mae: 0.0218 - mse: 0.0024\n",
      "Epoch 21: val_loss improved from 0.00107 to 0.00092, saving model to best_model_2.keras\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 775ms/step - loss: 9.4908e-04 - mae: 0.0218 - mse: 0.0024 - val_loss: 9.1627e-04 - val_mae: 0.0212 - val_mse: 0.0024 - learning_rate: 9.5098e-04\n",
      "Epoch 22/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 478ms/step - loss: 0.0012 - mae: 0.0244 - mse: 0.0032\n",
      "Epoch 22: val_loss did not improve from 0.00092\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 614ms/step - loss: 0.0012 - mae: 0.0244 - mse: 0.0032 - val_loss: 0.0017 - val_mae: 0.0361 - val_mse: 0.0037 - learning_rate: 9.4089e-04\n",
      "Epoch 23/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 478ms/step - loss: 7.3555e-04 - mae: 0.0209 - mse: 0.0017"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:13:01.597606: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 23: val_loss did not improve from 0.00092\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 610ms/step - loss: 7.3491e-04 - mae: 0.0209 - mse: 0.0017 - val_loss: 0.0010 - val_mae: 0.0207 - val_mse: 0.0026 - learning_rate: 9.2993e-04\n",
      "Epoch 24/80\n",
      "\u001b[1m72/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 481ms/step - loss: 6.3088e-04 - mae: 0.0189 - mse: 0.0015\n",
      "Epoch 24: val_loss did not improve from 0.00092\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 599ms/step - loss: 6.3161e-04 - mae: 0.0189 - mse: 0.0015 - val_loss: 0.0011 - val_mae: 0.0217 - val_mse: 0.0027 - learning_rate: 9.1811e-04\n",
      "Epoch 25/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 480ms/step - loss: 6.1305e-04 - mae: 0.0190 - mse: 0.0014"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:14:30.857950: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 25: val_loss improved from 0.00092 to 0.00074, saving model to best_model_2.keras\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 797ms/step - loss: 6.1308e-04 - mae: 0.0190 - mse: 0.0014 - val_loss: 7.4020e-04 - val_mae: 0.0171 - val_mse: 0.0020 - learning_rate: 9.0546e-04\n",
      "Epoch 26/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489ms/step - loss: 5.5808e-04 - mae: 0.0182 - mse: 0.0013\n",
      "Epoch 26: val_loss improved from 0.00074 to 0.00072, saving model to best_model_2.keras\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 911ms/step - loss: 5.5800e-04 - mae: 0.0182 - mse: 0.0013 - val_loss: 7.2292e-04 - val_mae: 0.0180 - val_mse: 0.0018 - learning_rate: 8.9201e-04\n",
      "Epoch 27/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484ms/step - loss: 5.8190e-04 - mae: 0.0182 - mse: 0.0013"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:16:45.904812: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 27: val_loss improved from 0.00072 to 0.00069, saving model to best_model_2.keras\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 784ms/step - loss: 5.8175e-04 - mae: 0.0182 - mse: 0.0013 - val_loss: 6.9481e-04 - val_mae: 0.0193 - val_mse: 0.0018 - learning_rate: 8.7777e-04\n",
      "Epoch 28/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483ms/step - loss: 5.5056e-04 - mae: 0.0173 - mse: 0.0013\n",
      "Epoch 28: val_loss did not improve from 0.00069\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 620ms/step - loss: 5.4997e-04 - mae: 0.0173 - mse: 0.0013 - val_loss: 0.0011 - val_mae: 0.0195 - val_mse: 0.0031 - learning_rate: 8.6278e-04\n",
      "Epoch 29/80\n",
      "\u001b[1m71/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 496ms/step - loss: 4.7130e-04 - mae: 0.0168 - mse: 0.0011"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:18:29.290144: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 29: val_loss did not improve from 0.00069\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 617ms/step - loss: 4.7394e-04 - mae: 0.0168 - mse: 0.0011 - val_loss: 0.0011 - val_mae: 0.0206 - val_mse: 0.0029 - learning_rate: 8.4708e-04\n",
      "Epoch 30/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483ms/step - loss: 4.6976e-04 - mae: 0.0162 - mse: 0.0011"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:19:14.855387: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 30: val_loss improved from 0.00069 to 0.00065, saving model to best_model_2.keras\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 743ms/step - loss: 4.7008e-04 - mae: 0.0162 - mse: 0.0011 - val_loss: 6.5233e-04 - val_mae: 0.0172 - val_mse: 0.0017 - learning_rate: 8.3068e-04\n",
      "Epoch 31/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485ms/step - loss: 5.2874e-04 - mae: 0.0172 - mse: 0.0012\n",
      "Epoch 31: val_loss did not improve from 0.00065\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 617ms/step - loss: 5.2794e-04 - mae: 0.0172 - mse: 0.0012 - val_loss: 0.0011 - val_mae: 0.0213 - val_mse: 0.0029 - learning_rate: 8.1363e-04\n",
      "Epoch 32/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 493ms/step - loss: 4.5353e-04 - mae: 0.0162 - mse: 0.0010\n",
      "Epoch 32: val_loss did not improve from 0.00065\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 635ms/step - loss: 4.5346e-04 - mae: 0.0162 - mse: 0.0010 - val_loss: 0.0012 - val_mae: 0.0240 - val_mse: 0.0030 - learning_rate: 7.9595e-04\n",
      "Epoch 33/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 480ms/step - loss: 3.7405e-04 - mae: 0.0155 - mse: 8.2884e-04\n",
      "Epoch 33: val_loss did not improve from 0.00065\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 616ms/step - loss: 3.7467e-04 - mae: 0.0155 - mse: 8.3047e-04 - val_loss: 9.6831e-04 - val_mae: 0.0187 - val_mse: 0.0024 - learning_rate: 7.7769e-04\n",
      "Epoch 34/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485ms/step - loss: 4.0000e-04 - mae: 0.0152 - mse: 9.0068e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:22:26.711194: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 34: val_loss improved from 0.00065 to 0.00064, saving model to best_model_2.keras\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 765ms/step - loss: 4.0044e-04 - mae: 0.0152 - mse: 9.0176e-04 - val_loss: 6.4202e-04 - val_mae: 0.0159 - val_mse: 0.0016 - learning_rate: 7.5889e-04\n",
      "Epoch 35/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488ms/step - loss: 3.7012e-04 - mae: 0.0153 - mse: 8.2237e-04\n",
      "Epoch 35: val_loss did not improve from 0.00064\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 648ms/step - loss: 3.7020e-04 - mae: 0.0152 - mse: 8.2257e-04 - val_loss: 6.7829e-04 - val_mae: 0.0162 - val_mse: 0.0017 - learning_rate: 7.3957e-04\n",
      "Epoch 36/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482ms/step - loss: 3.3972e-04 - mae: 0.0148 - mse: 7.3970e-04\n",
      "Epoch 36: val_loss did not improve from 0.00064\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 615ms/step - loss: 3.3981e-04 - mae: 0.0148 - mse: 7.3999e-04 - val_loss: 6.9830e-04 - val_mae: 0.0180 - val_mse: 0.0017 - learning_rate: 7.1977e-04\n",
      "Epoch 37/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 498ms/step - loss: 3.1341e-04 - mae: 0.0143 - mse: 6.8489e-04\n",
      "Epoch 37: val_loss did not improve from 0.00064\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 633ms/step - loss: 3.1343e-04 - mae: 0.0143 - mse: 6.8486e-04 - val_loss: 6.9991e-04 - val_mae: 0.0170 - val_mse: 0.0019 - learning_rate: 6.9955e-04\n",
      "Epoch 38/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488ms/step - loss: 2.5717e-04 - mae: 0.0137 - mse: 5.6673e-04\n",
      "Epoch 38: val_loss did not improve from 0.00064\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 669ms/step - loss: 2.5717e-04 - mae: 0.0137 - mse: 5.6679e-04 - val_loss: 7.4872e-04 - val_mae: 0.0185 - val_mse: 0.0019 - learning_rate: 6.7893e-04\n",
      "Epoch 39/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482ms/step - loss: 1.9478e-04 - mae: 0.0129 - mse: 3.9856e-04\n",
      "Epoch 39: val_loss did not improve from 0.00064\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 627ms/step - loss: 1.9512e-04 - mae: 0.0129 - mse: 3.9949e-04 - val_loss: 7.0962e-04 - val_mae: 0.0161 - val_mse: 0.0019 - learning_rate: 6.5796e-04\n",
      "Epoch 40/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 495ms/step - loss: 2.0410e-04 - mae: 0.0126 - mse: 4.4147e-04\n",
      "Epoch 40: val_loss did not improve from 0.00064\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 622ms/step - loss: 2.0434e-04 - mae: 0.0126 - mse: 4.4219e-04 - val_loss: 6.5726e-04 - val_mae: 0.0185 - val_mse: 0.0017 - learning_rate: 6.3669e-04\n",
      "Epoch 41/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489ms/step - loss: 1.7640e-04 - mae: 0.0122 - mse: 3.7456e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:28:04.316935: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 41: val_loss did not improve from 0.00064\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 615ms/step - loss: 1.7663e-04 - mae: 0.0122 - mse: 3.7514e-04 - val_loss: 6.6313e-04 - val_mae: 0.0158 - val_mse: 0.0017 - learning_rate: 6.1515e-04\n",
      "Epoch 42/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 481ms/step - loss: 1.5056e-04 - mae: 0.0121 - mse: 3.1254e-04\n",
      "Epoch 42: val_loss improved from 0.00064 to 0.00059, saving model to best_model_2.keras\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 765ms/step - loss: 1.5072e-04 - mae: 0.0121 - mse: 3.1299e-04 - val_loss: 5.9059e-04 - val_mae: 0.0144 - val_mse: 0.0015 - learning_rate: 5.9339e-04\n",
      "Epoch 43/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486ms/step - loss: 1.5833e-04 - mae: 0.0115 - mse: 3.3297e-04\n",
      "Epoch 43: val_loss did not improve from 0.00059\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 620ms/step - loss: 1.5849e-04 - mae: 0.0115 - mse: 3.3337e-04 - val_loss: 0.0012 - val_mae: 0.0213 - val_mse: 0.0030 - learning_rate: 5.7145e-04\n",
      "Epoch 44/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483ms/step - loss: 1.4578e-04 - mae: 0.0114 - mse: 3.0596e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:30:31.114370: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 44: val_loss did not improve from 0.00059\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 603ms/step - loss: 1.4599e-04 - mae: 0.0114 - mse: 3.0645e-04 - val_loss: 6.6093e-04 - val_mae: 0.0164 - val_mse: 0.0017 - learning_rate: 5.4937e-04\n",
      "Epoch 45/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483ms/step - loss: 1.5288e-04 - mae: 0.0119 - mse: 3.1516e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:31:15.503067: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 45: val_loss improved from 0.00059 to 0.00056, saving model to best_model_2.keras\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m56s\u001b[0m 767ms/step - loss: 1.5302e-04 - mae: 0.0119 - mse: 3.1559e-04 - val_loss: 5.5846e-04 - val_mae: 0.0131 - val_mse: 0.0014 - learning_rate: 5.2721e-04\n",
      "Epoch 46/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489ms/step - loss: 1.3778e-04 - mae: 0.0112 - mse: 2.8924e-04\n",
      "Epoch 46: val_loss did not improve from 0.00056\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 678ms/step - loss: 1.3781e-04 - mae: 0.0112 - mse: 2.8928e-04 - val_loss: 8.4344e-04 - val_mae: 0.0155 - val_mse: 0.0022 - learning_rate: 5.0500e-04\n",
      "Epoch 47/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482ms/step - loss: 1.4037e-04 - mae: 0.0110 - mse: 2.9758e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:33:04.974744: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 47: val_loss did not improve from 0.00056\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 604ms/step - loss: 1.4023e-04 - mae: 0.0110 - mse: 2.9725e-04 - val_loss: 5.7851e-04 - val_mae: 0.0125 - val_mse: 0.0015 - learning_rate: 4.8279e-04\n",
      "Epoch 48/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 485ms/step - loss: 1.1652e-04 - mae: 0.0108 - mse: 2.3837e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:33:49.518673: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 48: val_loss did not improve from 0.00056\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 616ms/step - loss: 1.1658e-04 - mae: 0.0108 - mse: 2.3857e-04 - val_loss: 5.8615e-04 - val_mae: 0.0128 - val_mse: 0.0016 - learning_rate: 4.6063e-04\n",
      "Epoch 49/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484ms/step - loss: 1.2777e-04 - mae: 0.0103 - mse: 2.8354e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:34:34.729361: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 49: val_loss improved from 0.00056 to 0.00055, saving model to best_model_2.keras\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 778ms/step - loss: 1.2754e-04 - mae: 0.0103 - mse: 2.8286e-04 - val_loss: 5.5203e-04 - val_mae: 0.0119 - val_mse: 0.0014 - learning_rate: 4.3855e-04\n",
      "Epoch 50/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469ms/step - loss: 1.0532e-04 - mae: 0.0104 - mse: 2.1391e-04\n",
      "Epoch 50: val_loss did not improve from 0.00055\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 596ms/step - loss: 1.0533e-04 - mae: 0.0104 - mse: 2.1397e-04 - val_loss: 6.1498e-04 - val_mae: 0.0139 - val_mse: 0.0016 - learning_rate: 4.1661e-04\n",
      "Epoch 51/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469ms/step - loss: 9.4650e-05 - mae: 0.0102 - mse: 1.9050e-04\n",
      "Epoch 51: val_loss did not improve from 0.00055\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 739ms/step - loss: 9.4656e-05 - mae: 0.0102 - mse: 1.9055e-04 - val_loss: 6.2291e-04 - val_mae: 0.0129 - val_mse: 0.0017 - learning_rate: 3.9485e-04\n",
      "Epoch 52/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 468ms/step - loss: 8.6376e-05 - mae: 0.0095 - mse: 1.7665e-04\n",
      "Epoch 52: val_loss did not improve from 0.00055\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 643ms/step - loss: 8.6397e-05 - mae: 0.0095 - mse: 1.7668e-04 - val_loss: 6.0495e-04 - val_mae: 0.0126 - val_mse: 0.0016 - learning_rate: 3.7331e-04\n",
      "Epoch 53/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 466ms/step - loss: 8.5298e-05 - mae: 0.0095 - mse: 1.7233e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:37:58.493335: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 53: val_loss improved from 0.00055 to 0.00051, saving model to best_model_2.keras\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 790ms/step - loss: 8.5314e-05 - mae: 0.0095 - mse: 1.7237e-04 - val_loss: 5.1300e-04 - val_mae: 0.0114 - val_mse: 0.0014 - learning_rate: 3.5204e-04\n",
      "Epoch 54/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471ms/step - loss: 7.5201e-05 - mae: 0.0092 - mse: 1.5048e-04\n",
      "Epoch 54: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 661ms/step - loss: 7.5252e-05 - mae: 0.0092 - mse: 1.5058e-04 - val_loss: 6.4774e-04 - val_mae: 0.0135 - val_mse: 0.0017 - learning_rate: 3.3107e-04\n",
      "Epoch 55/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482ms/step - loss: 7.5198e-05 - mae: 0.0093 - mse: 1.5040e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:39:47.668534: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 55: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 614ms/step - loss: 7.5221e-05 - mae: 0.0093 - mse: 1.5044e-04 - val_loss: 5.8899e-04 - val_mae: 0.0120 - val_mse: 0.0016 - learning_rate: 3.1045e-04\n",
      "Epoch 56/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 471ms/step - loss: 7.4847e-05 - mae: 0.0091 - mse: 1.5081e-04\n",
      "Epoch 56: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 656ms/step - loss: 7.4912e-05 - mae: 0.0091 - mse: 1.5097e-04 - val_loss: 5.8111e-04 - val_mae: 0.0122 - val_mse: 0.0016 - learning_rate: 2.9023e-04\n",
      "Epoch 57/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 482ms/step - loss: 6.9574e-05 - mae: 0.0089 - mse: 1.3916e-04\n",
      "Epoch 57: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 616ms/step - loss: 6.9589e-05 - mae: 0.0089 - mse: 1.3919e-04 - val_loss: 5.7993e-04 - val_mae: 0.0114 - val_mse: 0.0016 - learning_rate: 2.7044e-04\n",
      "Epoch 58/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 472ms/step - loss: 6.5859e-05 - mae: 0.0086 - mse: 1.3172e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:42:05.832373: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 58: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 604ms/step - loss: 6.5888e-05 - mae: 0.0086 - mse: 1.3178e-04 - val_loss: 6.9159e-04 - val_mae: 0.0133 - val_mse: 0.0019 - learning_rate: 2.5111e-04\n",
      "Epoch 59/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484ms/step - loss: 6.5885e-05 - mae: 0.0086 - mse: 1.3184e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:42:51.247383: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 59: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 620ms/step - loss: 6.5927e-05 - mae: 0.0086 - mse: 1.3192e-04 - val_loss: 6.4814e-04 - val_mae: 0.0126 - val_mse: 0.0018 - learning_rate: 2.3231e-04\n",
      "Epoch 60/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486ms/step - loss: 6.4764e-05 - mae: 0.0087 - mse: 1.2953e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:43:37.109803: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 60: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 608ms/step - loss: 6.4769e-05 - mae: 0.0087 - mse: 1.2954e-04 - val_loss: 6.3801e-04 - val_mae: 0.0120 - val_mse: 0.0017 - learning_rate: 2.1405e-04\n",
      "Epoch 61/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489ms/step - loss: 6.4388e-05 - mae: 0.0086 - mse: 1.2878e-04\n",
      "Epoch 61: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 629ms/step - loss: 6.4395e-05 - mae: 0.0086 - mse: 1.2879e-04 - val_loss: 6.2788e-04 - val_mae: 0.0120 - val_mse: 0.0017 - learning_rate: 1.9637e-04\n",
      "Epoch 62/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488ms/step - loss: 6.2488e-05 - mae: 0.0085 - mse: 1.2503e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:45:08.550827: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 62: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 617ms/step - loss: 6.2506e-05 - mae: 0.0085 - mse: 1.2507e-04 - val_loss: 5.5049e-04 - val_mae: 0.0116 - val_mse: 0.0015 - learning_rate: 1.7932e-04\n",
      "Epoch 63/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 488ms/step - loss: 6.2879e-05 - mae: 0.0084 - mse: 1.2691e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:45:53.872352: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 63: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 626ms/step - loss: 6.2915e-05 - mae: 0.0084 - mse: 1.2699e-04 - val_loss: 5.4832e-04 - val_mae: 0.0111 - val_mse: 0.0015 - learning_rate: 1.6292e-04\n",
      "Epoch 64/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 481ms/step - loss: 5.8482e-05 - mae: 0.0083 - mse: 1.1696e-04\n",
      "Epoch 64: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 599ms/step - loss: 5.8483e-05 - mae: 0.0083 - mse: 1.1697e-04 - val_loss: 5.7182e-04 - val_mae: 0.0119 - val_mse: 0.0016 - learning_rate: 1.4722e-04\n",
      "Epoch 65/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484ms/step - loss: 5.8023e-05 - mae: 0.0083 - mse: 1.1605e-04\n",
      "Epoch 65: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 628ms/step - loss: 5.8030e-05 - mae: 0.0083 - mse: 1.1606e-04 - val_loss: 5.6279e-04 - val_mae: 0.0111 - val_mse: 0.0015 - learning_rate: 1.3223e-04\n",
      "Epoch 66/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489ms/step - loss: 5.6459e-05 - mae: 0.0081 - mse: 1.1292e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:48:10.533547: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 66: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 629ms/step - loss: 5.6456e-05 - mae: 0.0081 - mse: 1.1291e-04 - val_loss: 5.7288e-04 - val_mae: 0.0111 - val_mse: 0.0016 - learning_rate: 1.1799e-04\n",
      "Epoch 67/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 478ms/step - loss: 5.7445e-05 - mae: 0.0082 - mse: 1.1489e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:48:55.960245: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 67: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 609ms/step - loss: 5.7419e-05 - mae: 0.0082 - mse: 1.1484e-04 - val_loss: 5.6678e-04 - val_mae: 0.0115 - val_mse: 0.0015 - learning_rate: 1.0454e-04\n",
      "Epoch 68/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 484ms/step - loss: 5.5758e-05 - mae: 0.0081 - mse: 1.1152e-04\n",
      "Epoch 68: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 771ms/step - loss: 5.5745e-05 - mae: 0.0081 - mse: 1.1149e-04 - val_loss: 5.6660e-04 - val_mae: 0.0113 - val_mse: 0.0015 - learning_rate: 9.1886e-05\n",
      "Epoch 69/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 486ms/step - loss: 5.4937e-05 - mae: 0.0080 - mse: 1.0987e-04\n",
      "Epoch 69: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 609ms/step - loss: 5.4932e-05 - mae: 0.0080 - mse: 1.0986e-04 - val_loss: 5.6774e-04 - val_mae: 0.0112 - val_mse: 0.0016 - learning_rate: 8.0068e-05\n",
      "Epoch 70/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 481ms/step - loss: 5.3157e-05 - mae: 0.0079 - mse: 1.0631e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:51:22.721426: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 70: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 611ms/step - loss: 5.3155e-05 - mae: 0.0079 - mse: 1.0631e-04 - val_loss: 5.7112e-04 - val_mae: 0.0111 - val_mse: 0.0016 - learning_rate: 6.9105e-05\n",
      "Epoch 71/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 469ms/step - loss: 5.7898e-05 - mae: 0.0081 - mse: 1.1673e-04\n",
      "Epoch 71: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 598ms/step - loss: 5.7878e-05 - mae: 0.0081 - mse: 1.1669e-04 - val_loss: 6.3989e-04 - val_mae: 0.0120 - val_mse: 0.0018 - learning_rate: 5.9020e-05\n",
      "Epoch 72/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 455ms/step - loss: 5.2956e-05 - mae: 0.0079 - mse: 1.0591e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:52:50.027661: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 72: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 585ms/step - loss: 5.2944e-05 - mae: 0.0079 - mse: 1.0589e-04 - val_loss: 6.2892e-04 - val_mae: 0.0116 - val_mse: 0.0017 - learning_rate: 4.9834e-05\n",
      "Epoch 73/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 501ms/step - loss: 5.4401e-05 - mae: 0.0078 - mse: 1.0881e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:53:36.590976: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 73: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 636ms/step - loss: 5.4386e-05 - mae: 0.0078 - mse: 1.0878e-04 - val_loss: 6.3080e-04 - val_mae: 0.0115 - val_mse: 0.0017 - learning_rate: 4.1564e-05\n",
      "Epoch 74/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 483ms/step - loss: 5.4271e-05 - mae: 0.0079 - mse: 1.0857e-04\n",
      "Epoch 74: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 608ms/step - loss: 5.4255e-05 - mae: 0.0079 - mse: 1.0854e-04 - val_loss: 6.2137e-04 - val_mae: 0.0115 - val_mse: 0.0017 - learning_rate: 3.4227e-05\n",
      "Epoch 75/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 489ms/step - loss: 5.2406e-05 - mae: 0.0079 - mse: 1.0481e-04\n",
      "Epoch 75: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m50s\u001b[0m 685ms/step - loss: 5.2401e-05 - mae: 0.0079 - mse: 1.0480e-04 - val_loss: 6.2219e-04 - val_mae: 0.0114 - val_mse: 0.0017 - learning_rate: 2.7838e-05\n",
      "Epoch 76/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 481ms/step - loss: 5.0337e-05 - mae: 0.0077 - mse: 1.0067e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:55:57.176298: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 76: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 614ms/step - loss: 5.0346e-05 - mae: 0.0077 - mse: 1.0069e-04 - val_loss: 6.2865e-04 - val_mae: 0.0117 - val_mse: 0.0017 - learning_rate: 2.2411e-05\n",
      "Epoch 77/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 472ms/step - loss: 5.4378e-05 - mae: 0.0079 - mse: 1.0878e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:56:41.841272: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 77: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 588ms/step - loss: 5.4363e-05 - mae: 0.0079 - mse: 1.0875e-04 - val_loss: 6.3824e-04 - val_mae: 0.0115 - val_mse: 0.0018 - learning_rate: 1.7955e-05\n",
      "Epoch 78/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 464ms/step - loss: 5.0795e-05 - mae: 0.0077 - mse: 1.0159e-04\n",
      "Epoch 78: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 745ms/step - loss: 5.0785e-05 - mae: 0.0077 - mse: 1.0157e-04 - val_loss: 6.3806e-04 - val_mae: 0.0115 - val_mse: 0.0018 - learning_rate: 1.4480e-05\n",
      "Epoch 79/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 467ms/step - loss: 5.0470e-05 - mae: 0.0077 - mse: 1.0094e-04"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-26 06:58:19.634999: W tensorflow/core/kernels/data/prefetch_autotuner.cc:52] Prefetch autotuner tried to allocate 60212224 bytes after encountering the first element of size 60212224 bytes.This already causes the autotune ram budget to be exceeded. To stay within the ram budget, either increase the ram budget or reduce element size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 79: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 595ms/step - loss: 5.0481e-05 - mae: 0.0077 - mse: 1.0096e-04 - val_loss: 6.3951e-04 - val_mae: 0.0115 - val_mse: 0.0018 - learning_rate: 1.1993e-05\n",
      "Epoch 80/80\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 490ms/step - loss: 4.9117e-05 - mae: 0.0076 - mse: 9.8234e-05\n",
      "Epoch 80: val_loss did not improve from 0.00051\n",
      "\u001b[1m73/73\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 630ms/step - loss: 4.9132e-05 - mae: 0.0076 - mse: 9.8265e-05 - val_loss: 6.4056e-04 - val_mae: 0.0116 - val_mse: 0.0018 - learning_rate: 1.0498e-05\n"
     ]
    }
   ],
   "source": [
    "with experiment.train():\n",
    "    history = model.fit(train_gen, epochs = 80, validation_data=val_gen, callbacks=[lr_callback, tensorboard_callback, model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # save the model as tfjs\n",
    "# model.save('iris_detection_modelV1_eff5.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: effb3_18_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: effb3_18_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at 'effb3_18_model/'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 4), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  140165251646864: TensorSpec(shape=(1, 1, 1, 3), dtype=tf.float32, name=None)\n",
      "  140165251648208: TensorSpec(shape=(1, 1, 1, 3), dtype=tf.float32, name=None)\n",
      "  140165251650704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251649552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251647248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251650320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251650896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251649936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251651664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251649744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251651856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251650128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251655120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251654352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251656080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251654928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251656848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251655888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251654736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251655696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251656464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251658768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251654544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251658384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251657808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251658192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251660880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251656656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251661648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251659920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165251660496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252137424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252137040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252137616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252137232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252140496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252139920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252139536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252139728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252140112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252142416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252141840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252141072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252137808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252142032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252144336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252141456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252145104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252143952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252145872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252144912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252142992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252144720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252145488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252147792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252147216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252146448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252143376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252147408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252149712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252149136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252148368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252146832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252149328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252151632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252148752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252152400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252151248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252153168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252152016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252546640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252150672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252152784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252548752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252548176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252547792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252547984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252548368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252550672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252550096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252549328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252546832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252550288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252552592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252549712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252553360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252552208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252554128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252553168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252551248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252552976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252553744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252556432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252555856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252555472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252555664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252556048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252558352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252557776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252557008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252553936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252557968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252560272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252557392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252561040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252559888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252561808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252560848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252558928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252560656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252561424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165252561616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080220112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080219920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080220304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080219728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080222416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080221840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080221072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080220496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080222032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080224336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080221456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080225104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080223952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080225872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080224912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080222992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080224720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080225488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080228176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080227600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080227216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080227408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080227792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080230096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080229520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080228752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080225680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080229712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080232016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080229136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080232784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080231632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080233552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080232592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080230672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080232400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080233168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080235856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080235088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080678480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080233360: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080235472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080680208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080679632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080679056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080678864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080679824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080682128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080679248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080682896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080681744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080683664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080682704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080680784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080682512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080683280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080685584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080685008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080684240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080681168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080685200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080687504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080686928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080686160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080684624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080687120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080689424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080686544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080690192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080689040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080690960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080690000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080688080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080689808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080690576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080693264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080692688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080692304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080692496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080692880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080694608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969316432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969316816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140165080690768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969317008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969318736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969316624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969319504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969318352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969320272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969319312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969317392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969319120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969319888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969322576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969322000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969321616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969321808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969322192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969324496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969323920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969323152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969320080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969324112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969326416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969323536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969327184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969326032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969327952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969326992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969325072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969326800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969327568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969330256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969329680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969329296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969329488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969329872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969332176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969331216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969330832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969327760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969331792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969776528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969775760: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969777296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969776144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969778064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969777104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969775568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969776912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969777680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969780560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969779984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969779600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969779792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969780176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969782480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969781904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969781136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969778640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969782096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969784400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969781520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969785168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969784016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969785936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969784976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969783056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969784784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969785552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969787856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969787280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969786512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969783440: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969787472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969789776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969789200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969788432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969786896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969789392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969791312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969790736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164969788816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916707984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916709136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916708176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916708560: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916707600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916708752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916711632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916711056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916710672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916710864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916711248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916713552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916712976: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916712208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916709712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916713168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916715472: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916712592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916716240: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916715088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916717008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916716048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916714128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916715856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916716624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916719312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916718736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916718352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916718544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916718928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916721232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916720656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916719888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916716816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916720848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916723152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916720272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916723536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916722192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164916722768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917182928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917182544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917183120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917182736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917185808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917185232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917184848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917185040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917185424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917187728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917187152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917186384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917183696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917187344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917189648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917186768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917190416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917189264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917191184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917190224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917188304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917190032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917190800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917193488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917192912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917192528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917192720: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917193104: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917195408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917194832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917194064: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917190992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917195024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917197328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917194448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917198096: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917196944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917198480: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646060688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646060496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917197136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164917196368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646062032: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646061456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646060880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646060112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646061648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646063952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646063376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646062608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646061072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646063568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646065872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646062992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646066640: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646065488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646067408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646066448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646064528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646066256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646067024: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646069712: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646069136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646068752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646068944: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646069328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646071632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646071056: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646070288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646067216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646071248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646073552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646070672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646074320: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646073168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646075088: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646074128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646072208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646073936: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646074704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646072592: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646536208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646536400: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646535824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646535248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646538128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646537552: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646536784: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646535632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646537744: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646540048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646537168: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646540816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646539664: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646541584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646540624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646538704: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646540432: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646541200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646543888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646543312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646542928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646543120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646543504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646545808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646545232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646544464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646541392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646545424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646547728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646544848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646548496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646547344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646549264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646548304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646546384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646548112: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646548880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646551184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646961808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646961616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646546768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646549072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646963152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646962576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646962000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646961232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646962768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646965072: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646962192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646965840: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646964688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646966608: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646965648: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646963728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646965456: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646966224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646968912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646968336: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646967952: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646968144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646968528: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646970832: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646970256: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646969488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646966416: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646970448: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646972752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646969872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646973520: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646972368: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646974288: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646973328: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646971408: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646973136: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646973904: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646976208: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646975632: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646974864: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646971792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646975824: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646975248: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882959120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882959504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140164646976016: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882959696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882961424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882958928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882962192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882961040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882962960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882962000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882960080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882961808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882962576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882965264: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882964688: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882964304: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882964496: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882964880: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882966224: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882967376: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882967568: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882966992: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882966800: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882967184: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882968912: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882969680: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882969872: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882969296: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882962768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882969488: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882971216: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882971984: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882972176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882971600: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882968144: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882971792: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882972752: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  140163882974672: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p effb3_18_model\n",
    "model.export('effb3_18_model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the loss\n",
    "plt.plot(history.history['loss'], label='train')\n",
    "plt.plot(history.history['val_loss'], label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 719ms/step - loss: 3.3023e-04 - mae: 0.0077 - mse: 7.4664e-04\n",
      "Test loss: [0.0005207324284128845, 0.0011767965042963624, 0.009711484424769878]\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "test_loss = model.evaluate(test_gen)\n",
    "print(\"Test loss:\", test_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # camera testing\n",
    "# def predict_camera_image(image):\n",
    "#     image = tf.image.resize(image, [256, 256])\n",
    "#     image = tf.cast(image, tf.float16) / 255.0\n",
    "#     image = tf.expand_dims(image, axis=0)\n",
    "#     prediction = model.predict(image)\n",
    "#     return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # open the camera\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "    \n",
    "#     # Preprocess the frame\n",
    "#     frame = cv2.resize(frame, (256, 256))\n",
    "#     frame = frame.astype(np.float16) / 255.0\n",
    "    \n",
    "#     # Make prediction\n",
    "#     prediction = predict_camera_image(frame)\n",
    "    \n",
    "#     # Correct coordinates\n",
    "#     coord1 = correct_coordinates(prediction[0, :2])\n",
    "#     coord2 = correct_coordinates(prediction[0, 2:])\n",
    "    \n",
    "#     # Draw circles on the frame\n",
    "#     cv2.circle(frame, coord1, 4, (255, 0, 0), -1)\n",
    "#     cv2.circle(frame, coord2, 4, (0, 255, 0), -1)\n",
    "    \n",
    "#     # Display the frame\n",
    "#     cv2.imshow('Real-time Prediction', frame)\n",
    "    \n",
    "#     # Break the loop if 'q' is pressed\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # Release the capture and destroy all windows\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
